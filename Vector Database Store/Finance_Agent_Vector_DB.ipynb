{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrbNjbxo3-x3",
        "outputId": "0e423666-a8f2-450e-e49b-7becedc885df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data.zip\n",
            "   creating: data/\n",
            "   creating: data/api_ninjas/\n",
            "  inflating: data/api_ninjas/cryptoprice.csv  \n",
            "  inflating: data/api_ninjas/marketcap.csv  \n",
            "  inflating: data/api_ninjas/stockprice.csv  \n",
            "   creating: data/macro/\n",
            "  inflating: data/macro/cpi.jsonl    \n",
            "   creating: data/news/\n",
            "  inflating: data/news/google_news.csv  \n",
            "   creating: data/news_articles/\n",
            "  inflating: data/news_articles/articles.jsonl  \n",
            "   creating: data/news_gdelt/\n",
            "  inflating: data/news_gdelt/gdelt_news.csv  \n",
            "   creating: data/prices/\n",
            "  inflating: data/prices/HDFCBANK_NS.parquet  \n",
            "  inflating: data/prices/ICICIBANK_NS.parquet  \n",
            "  inflating: data/prices/INFY_NS.parquet  \n",
            "  inflating: data/prices/RELIANCE_NS.parquet  \n",
            "  inflating: data/prices/TCS_NS.parquet  \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/data.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab: ensure GPU is on (Runtime → Change runtime type → GPU)\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3m4zF0M4gy3",
        "outputId": "82815d05-556f-427d-c57f-c41a96372ade"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 14 15:41:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.4/136.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep numpy pinned, upgrade hnswlib to a compatible build\n",
        "!pip -q install -U \"pip<24.3\"\n",
        "!pip -q uninstall -y numpy hnswlib chromadb > /dev/null\n",
        "!pip -q install \"numpy==1.26.4\"\n",
        "!pip -q install \"hnswlib==0.8.1\" \"chromadb==0.5.5\"\n",
        "!pip -q install \"sentence-transformers==3.0.1\" \"pandas==2.2.2\" \"tqdm==4.66.5\" \"jsonlines==4.0.0\"\n",
        "\n",
        "import os, time\n",
        "print(\"✅ Installed compatible versions. Restarting runtime…\")\n",
        "time.sleep(1)\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ML3h4mD5XP_",
        "outputId": "d1e325f5-95f1-4c06-fa97-1a06c40dc625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement hnswlib==0.8.1 (from versions: 0.3.2.0, 0.3.4, 0.4.0, 0.5.0, 0.5.1, 0.5.2, 0.6.0, 0.6.1, 0.6.2, 0.7.0, 0.8.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for hnswlib==0.8.1\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Installed compatible versions. Restarting runtime…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install hnswlib chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiDh2u9O9cwh",
        "outputId": "c438f9cf-8df1-4b14-8620-5bbb4da0925b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m142.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers"
      ],
      "metadata": {
        "id": "nt-kbORT8LOM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, jsonlines, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATA_DIR = \"/content/data\"\n",
        "ART_JSONL = os.path.join(DATA_DIR, \"news_articles\", \"articles.jsonl\")\n",
        "GDELT_CSV = os.path.join(DATA_DIR, \"news_gdelt\", \"gdelt_news.csv\")\n",
        "GNEWS_CSV = os.path.join(DATA_DIR, \"news\", \"google_news.csv\")\n",
        "\n",
        "PERSIST_DIR = \"/content/vectorstore/news_v1\"\n",
        "COLLECTION_NAME = \"india_news_v1\"\n",
        "\n",
        "for p in [ART_JSONL, GDELT_CSV, GNEWS_CSV]:\n",
        "    print(\"OK\" if os.path.exists(p) else \"MISSING\", \"→\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9efwPzn4lxw",
        "outputId": "2dc49492-5405-4d7d-cc00-75d3645744c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK → /content/data/news_articles/articles.jsonl\n",
            "OK → /content/data/news_gdelt/gdelt_news.csv\n",
            "OK → /content/data/news/google_news.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_articles_jsonl(path):\n",
        "    out = []\n",
        "    if not os.path.exists(path): return out\n",
        "    with jsonlines.open(path, \"r\") as reader:\n",
        "        for obj in reader:\n",
        "            text = (obj.get(\"text\") or \"\").strip()\n",
        "            if not text: continue\n",
        "            out.append({\n",
        "                \"url\": (obj.get(\"url\") or \"\").strip(),\n",
        "                \"title\": (obj.get(\"title\") or \"\").strip(),\n",
        "                \"published\": (obj.get(\"seendate\") or \"\").strip(),\n",
        "                \"domain\": (obj.get(\"domain\") or \"\").strip(),\n",
        "                \"source\": \"articles.jsonl\",\n",
        "                \"text\": text\n",
        "            })\n",
        "    return [r for r in out if r[\"url\"] and r[\"text\"]]\n",
        "\n",
        "def load_news_csv(path, title_col=\"title\", link_col=\"url\", published_col=\"published\"):\n",
        "    out = []\n",
        "    if not os.path.exists(path): return out\n",
        "    df = pd.read_csv(path)\n",
        "    # normalize col names\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    tcol = cols.get(title_col, cols.get(\"title\"))\n",
        "    lcol = cols.get(link_col, cols.get(\"link\", cols.get(\"resolved_url\", \"url\")))\n",
        "    pcol = cols.get(published_col, cols.get(\"seendate\", cols.get(\"published\", \"published_at\")))\n",
        "    for _, r in df.iterrows():\n",
        "        url = str(r.get(lcol, \"\")).strip() if lcol in r else \"\"\n",
        "        title = str(r.get(tcol, \"\")).strip() if tcol in r else \"\"\n",
        "        published = str(r.get(pcol, \"\")).strip() if pcol in r else \"\"\n",
        "        if not url or not title: continue\n",
        "        out.append({\n",
        "            \"url\": url, \"title\": title, \"published\": published,\n",
        "            \"domain\": str(r.get(\"domain\",\"\")).strip(),\n",
        "            \"source\": os.path.basename(path),\n",
        "            \"text\": title  # fallback text\n",
        "        })\n",
        "    return out\n",
        "\n",
        "art_recs   = load_articles_jsonl(ART_JSONL)\n",
        "gdelt_recs = load_news_csv(GDELT_CSV, title_col=\"title\", link_col=\"url\", published_col=\"seendate\")\n",
        "gnews_recs = load_news_csv(GNEWS_CSV, title_col=\"title\", link_col=\"link\", published_col=\"published\")\n",
        "\n",
        "len(art_recs), len(gdelt_recs), len(gnews_recs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFT7IifQ45Jx",
        "outputId": "5610eb26-14f2-405b-b514-4c257aa40cb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 21894, 337)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def dedup_keep_longest(*lists):\n",
        "    bucket = defaultdict(list)\n",
        "    for L in lists:\n",
        "        for r in L:\n",
        "            bucket[r[\"url\"]].append(r)\n",
        "    out = []\n",
        "    for url, items in bucket.items():\n",
        "        out.append(max(items, key=lambda x: len(x.get(\"text\",\"\"))))\n",
        "    return out\n",
        "\n",
        "records = dedup_keep_longest(art_recs, gdelt_recs, gnews_recs)\n",
        "# Keep full-text; add at most 2k headline-only items to improve coverage\n",
        "full = [r for r in records if len(r[\"text\"]) >= 200]\n",
        "short = [r for r in records if len(r[\"text\"]) < 200][:2000]\n",
        "records = full + short\n",
        "len(records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts4dim935wKO",
        "outputId": "5b2f266e-bd06-4feb-d58d-28882407af09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2519"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install once if transformers isn't present (Colab usually has it via sentence-transformers)\n",
        "# !pip -q install transformers\n",
        "\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use the same model family as your embedder for token counting\n",
        "tok = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "_sentence_splitter = re.compile(r'(?<=[\\.\\?\\!])\\s+(?=[A-Z0-9“\"])')\n",
        "\n",
        "def split_into_sentences(text: str):\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # quickly bail if very short\n",
        "    if len(text) < 400:\n",
        "        return [text]\n",
        "    parts = _sentence_splitter.split(text)\n",
        "    # merge tiny fragments back to previous sentence\n",
        "    merged = []\n",
        "    for p in parts:\n",
        "        if merged and len(p) < 40:\n",
        "            merged[-1] += \" \" + p\n",
        "        else:\n",
        "            merged.append(p)\n",
        "    return merged\n",
        "\n",
        "def chunk_by_tokens(sentences, max_tokens=256, overlap_tokens=32):\n",
        "    chunks, cur, cur_tokens = [], [], 0\n",
        "    for s in sentences:\n",
        "        t = len(tok.encode(s, add_special_tokens=False))\n",
        "        if t > max_tokens:  # very long sentence → hard-split by commas/spaces\n",
        "            subparts = re.split(r'([,;:–-])', s)\n",
        "            for sp in subparts:\n",
        "                tt = len(tok.encode(sp, add_special_tokens=False))\n",
        "                if cur_tokens + tt <= max_tokens:\n",
        "                    cur.append(sp); cur_tokens += tt\n",
        "                else:\n",
        "                    if cur: chunks.append(\"\".join(cur).strip())\n",
        "                    # start new with overlap from tail of previous\n",
        "                    if overlap_tokens and chunks:\n",
        "                        # take tail tokens from previous chunk\n",
        "                        tail = tok.encode(chunks[-1], add_special_tokens=False)[-overlap_tokens:]\n",
        "                        tail_text = tok.decode(tail, skip_special_tokens=True)\n",
        "                        cur, cur_tokens = [tail_text, sp], len(tail) + tt\n",
        "                    else:\n",
        "                        cur, cur_tokens = [sp], tt\n",
        "            continue\n",
        "\n",
        "        # normal sentence\n",
        "        if cur_tokens + t <= max_tokens:\n",
        "            cur.append(s); cur_tokens += t\n",
        "        else:\n",
        "            if cur: chunks.append(\" \".join(cur).strip())\n",
        "            # overlap\n",
        "            tail = []\n",
        "            if overlap_tokens and chunks:\n",
        "                tail_tok = tok.encode(chunks[-1], add_special_tokens=False)[-overlap_tokens:]\n",
        "                tail = [tok.decode(tail_tok, skip_special_tokens=True)]\n",
        "                cur_tokens = len(tail_tok)\n",
        "            else:\n",
        "                cur_tokens = 0\n",
        "            cur = tail + [s]\n",
        "            cur_tokens += t\n",
        "\n",
        "    if cur: chunks.append(\" \".join(cur).strip())\n",
        "    return chunks\n",
        "\n",
        "def recursive_sentence_token_chunk(text, max_tokens=256, overlap_tokens=32):\n",
        "    sents = split_into_sentences(text)\n",
        "    return chunk_by_tokens(sents, max_tokens=max_tokens, overlap_tokens=overlap_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vHsm9sZ7iNF",
        "outputId": "7af7dafa-f20b-4412-cdd9-bf45a4ac357f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs, metas, ids = [], [], []\n",
        "for idx, r in enumerate(records):  # records loaded from articles.jsonl\n",
        "    chunks = recursive_sentence_token_chunk(r[\"text\"], max_tokens=256, overlap_tokens=32)\n",
        "    for j, ch in enumerate(chunks):\n",
        "        docs.append(ch)\n",
        "        metas.append({\n",
        "            \"url\": r[\"url\"], \"title\": r[\"title\"], \"published\": r[\"published\"],\n",
        "            \"domain\": r[\"domain\"], \"source\": r[\"source\"], \"chunk\": j\n",
        "        })\n",
        "        ids.append(f\"{idx}:{j}\")\n",
        "\n",
        "print(\"chunks:\", len(docs), \"avg_len_tokens≈\",\n",
        "      sum(len(tok.encode(d, add_special_tokens=False)) for d in docs)//max(1,len(docs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGEjsTpC76Zg",
        "outputId": "1727e369-08fd-46a0-fe3b-f572d42ff4d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunks: 3544 avg_len_tokens≈ 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embed_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", device=device)\n",
        "\n",
        "client = chromadb.PersistentClient(path=PERSIST_DIR, settings=Settings(anonymized_telemetry=False))\n",
        "# clean rebuild\n",
        "try: client.delete_collection(COLLECTION_NAME)\n",
        "except: pass\n",
        "collection = client.create_collection(name=COLLECTION_NAME, metadata={\"hnsw:space\":\"cosine\"})\n",
        "\n",
        "BATCH = 512\n",
        "for i in tqdm(range(0, len(docs), BATCH)):\n",
        "    batch_docs = docs[i:i+BATCH]\n",
        "    batch_ids  = ids[i:i+BATCH]\n",
        "    batch_m    = metas[i:i+BATCH]\n",
        "    embs = embed_model.encode(batch_docs, batch_size=64, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=False)\n",
        "    collection.add(documents=batch_docs, embeddings=embs, metadatas=batch_m, ids=batch_ids)\n",
        "\n",
        "print(\"✅ Chroma build complete. Persisted at:\", PERSIST_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBPRrqY88ZTR",
        "outputId": "2d7c33f6-c772-4a8d-bb4a-762b07134f0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:18<00:00,  2.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chroma build complete. Persisted at: /content/vectorstore/news_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search(q, k=5):\n",
        "    q_emb = embed_model.encode([q], normalize_embeddings=True, convert_to_numpy=True)\n",
        "    res = collection.query(query_embeddings=q_emb, n_results=k, include=[\"metadatas\",\"documents\",\"distances\"])\n",
        "    for i in range(len(res[\"ids\"][0])):\n",
        "        score = 1 - res[\"distances\"][0][i]\n",
        "        md = res[\"metadatas\"][0][i]\n",
        "        print(f\"\\n[{i+1}] score={score:.4f} | {md.get('title','')}\\n{md.get('url','')}\\n{res['documents'][0][i][:240]} ...\")\n",
        "\n",
        "search(\"impact of RBI repo rate hikes on Indian bank margins\", k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4Xs2hMB8jRo",
        "outputId": "16cccbee-7368-4ef4-c237-84f52a410761"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1] score=0.8295 | RBI Repo Rate cut : A golden opportunity for homebuyers\n",
            "http://www.dailypioneer.com/2025/columnists/rbi---s-repo-rate-cut--a-golden-opportunity-for-homebuyers.html\n",
            "RBI Repo Rate cut : A golden opportunity for homebuyers ...\n",
            "\n",
            "[2] score=0.8230 | Retail inflation to come below RBI 4 . 4 % estimates in Jan - Mar quarter at 3 . 8 %: Report\n",
            "https://economictimes.indiatimes.com/news/economy/indicators/retail-inflation-to-come-below-rbis-4-4-estimates-in-jan-mar-quarter-at-3-8-report/articleshow/119000696.cms\n",
            ", including advanced economies, but india has largely managed to steer its inflation trajectory quite well. the rbi had kept the repo rate elevated to keep inflation contained. The repo rate is the rate of interest at which the RBI lends to ...\n",
            "\n",
            "[3] score=0.8188 | Check latest loan interest rates after RBI repo rate cut\n",
            "https://www.moneycontrol.com/news/business/personal-finance/these-banks-have-revised-personal-loan-interest-rates-after-rbi-repo-rate-cut-check-full-list-12956167.html\n",
            "07 March, 2025 | 11:01 IST After the Reserve Bank of India (RBI) announced a 25 basis point reduction in the repo rate to 6.25% on February 7, loan borrowers are expected to see some relief. This marks the RBI’s first rate cut in five years ...\n",
            "\n",
            "[4] score=0.8187 | Retail inflation to come below RBI 4 . 4 % estimates in Jan - Mar quarter at 3 . 8 %: Report\n",
            "https://economictimes.indiatimes.com/news/economy/indicators/retail-inflation-to-come-below-rbis-4-4-estimates-in-jan-mar-quarter-at-3-8-report/articleshow/119000696.cms\n",
            "rbi had kept the repo rate elevated to keep inflation contained. the repo rate is the rate of interest at which the rbi lends to other banks. India's retail inflation significantly improved in February 2025, as the year-on-year Consumer Pri ...\n",
            "\n",
            "[5] score=0.8160 | Kaushik Das : Expect another RBI rate cut in support of economic growth\n",
            "https://www.livemint.com/opinion/online-views/kaushik-das-expect-another-rbi-rate-cut-in-support-of-economic-growth-sanjay-malhotra-repo-rate-interest-rate-crr-vrr-11740638380241.html\n",
            "Kaushik Das: Expect another RBI rate cut in support of economic growth - This April may see the Indian central bank’s monetary policy panel cut its repo rate by 25 basis points. Current economic dynamics also suggest we’ll see a strategic c ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/vectorstore.zip /content/vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDiVOuvQ-Hrw",
        "outputId": "19c8889f-7676-4e6c-fde8-ad8aaa399d40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/vectorstore/ (stored 0%)\n",
            "  adding: content/vectorstore/news_v1/ (stored 0%)\n",
            "  adding: content/vectorstore/news_v1/ac3316c6-0efa-4eba-9b37-fdbdacddeccc/ (stored 0%)\n",
            "  adding: content/vectorstore/news_v1/ac3316c6-0efa-4eba-9b37-fdbdacddeccc/index_metadata.pickle (deflated 65%)\n",
            "  adding: content/vectorstore/news_v1/ac3316c6-0efa-4eba-9b37-fdbdacddeccc/link_lists.bin (deflated 84%)\n",
            "  adding: content/vectorstore/news_v1/ac3316c6-0efa-4eba-9b37-fdbdacddeccc/header.bin (deflated 61%)\n",
            "  adding: content/vectorstore/news_v1/ac3316c6-0efa-4eba-9b37-fdbdacddeccc/length.bin (deflated 85%)\n",
            "  adding: content/vectorstore/news_v1/ac3316c6-0efa-4eba-9b37-fdbdacddeccc/data_level0.bin (deflated 13%)\n",
            "  adding: content/vectorstore/news_v1/chroma.sqlite3 (deflated 54%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/vectorstore.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bnWO0w2n-Pbv",
        "outputId": "cb40b5fe-8d16-4d27-8b0c-b39dd9e6b662"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_36772220-5a41-429e-b7cf-3f47c9a991cf\", \"vectorstore.zip\", 13763627)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}