{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d47fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import feedparser\n",
    "import tldextract\n",
    "import yaml\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e14285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Paths --------\n",
    "BASE_DIR = 'C:\\\\Users\\\\harsh\\\\OneDrive\\\\Desktop\\\\LLM Capstone\\\\Data Collection'\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "PRICES_DIR = os.path.join(DATA_DIR, \"prices\")\n",
    "MACRO_DIR = os.path.join(DATA_DIR, \"macro\")\n",
    "NEWS_DIR = os.path.join(DATA_DIR, \"news\")\n",
    "CONFIGS_DIR = os.path.join(BASE_DIR, \"configs\")\n",
    "\n",
    "os.makedirs(PRICES_DIR, exist_ok=True)\n",
    "os.makedirs(MACRO_DIR, exist_ok=True)\n",
    "os.makedirs(NEWS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da939b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Env --------\n",
    "load_dotenv()\n",
    "API_NINJAS_KEY = os.getenv(\"API_NINJAS_KEY\", \"\").strip()\n",
    "DATA_GOV_IN_KEY = os.getenv(\"DATA_GOV_IN_KEY\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e77d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Helpers --------\n",
    "def read_yaml(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def save_parquet(df: pd.DataFrame, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "def append_jsonl(records, path: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def read_jsonl(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                out.append(json.loads(line))\n",
    "    return out\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_pub_date(dt_str):\n",
    "    if not dt_str:\n",
    "        return None\n",
    "    # dateparser handles a lot of RSS formats\n",
    "    dt = dateparser.parse(dt_str)\n",
    "    if dt is None:\n",
    "        return None\n",
    "    # make ISO with Z\n",
    "    return dt.astimezone(timezone.utc).replace(tzinfo=timezone.utc).isoformat().replace(\"+00:00\", \"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920429eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[prices] INFY.NS: saved 744 rows\n",
      "[prices] RELIANCE.NS: saved 744 rows\n",
      "[prices] HDFCBANK.NS: saved 744 rows\n",
      "[prices] TCS.NS: saved 744 rows\n",
      "[prices] ICICIBANK.NS: saved 744 rows\n"
     ]
    }
   ],
   "source": [
    "TICKERS = [\"INFY.NS\", \"RELIANCE.NS\", \"HDFCBANK.NS\", \"TCS.NS\", \"ICICIBANK.NS\"]\n",
    "\n",
    "for symbol in TICKERS:\n",
    "    try:\n",
    "        df = yf.Ticker(symbol).history(period=\"3y\", interval=\"1d\", auto_adjust=False)\n",
    "        if df.empty:\n",
    "            df = yf.Ticker(symbol).history(period=\"max\", interval=\"1d\", auto_adjust=False)\n",
    "        if df.empty:\n",
    "            print(f\"[prices] {symbol}: no data, skipping\")\n",
    "            continue\n",
    "\n",
    "        df = df.rename_axis(\"date\").reset_index()\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "        # standardize names\n",
    "        df = df.rename(columns={\n",
    "            \"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\"Close\":\"close\",\n",
    "            \"Adj Close\":\"adj_close\",\"Volume\":\"volume\"\n",
    "        })\n",
    "        if \"adj_close\" not in df.columns:\n",
    "            df[\"adj_close\"] = df[\"close\"]\n",
    "\n",
    "        df[\"symbol\"] = symbol\n",
    "        out_path = os.path.join(PRICES_DIR, f\"{symbol.replace('.','_')}.parquet\")\n",
    "        df.to_parquet(out_path, index=False)\n",
    "        print(f\"[prices] {symbol}: saved {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"[prices] {symbol}: ERROR → {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21fa2ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), \"data\", \"api_ninjas\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "HEADERS = {\"X-Api-Key\": API_NINJAS_KEY}\n",
    "\n",
    "NSE_TICKERS = [\"INFY\", \"RELIANCE\", \"HDFCBANK\", \"TCS\", \"ICICIBANK\"]\n",
    "\n",
    "def fetch_stockprice(symbols=NSE_TICKERS):\n",
    "    \"\"\"Fetch current price using /v1/stockprice.\"\"\"\n",
    "    out_path = os.path.join(DATA_DIR, \"stockprice.csv\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ticker,name,price,exchange,currency,updated\\n\")\n",
    "        for s in symbols:\n",
    "            url = f\"https://api.api-ninjas.com/v1/stockprice?ticker={s}\"\n",
    "            try:\n",
    "                r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "                if r.status_code != 200:\n",
    "                    print(f\"[stockprice] {s}: status {r.status_code}\")\n",
    "                    continue\n",
    "                data = r.json()\n",
    "                # data example: { \"ticker\": \"AAPL\", \"name\": \"Apple Inc.\", ... }\n",
    "                ticker = data.get(\"ticker\", \"\")\n",
    "                name = data.get(\"name\", \"\")\n",
    "                price = data.get(\"price\", \"\")\n",
    "                exchange = data.get(\"exchange\", \"\")\n",
    "                currency = data.get(\"currency\", \"\")\n",
    "                updated = data.get(\"updated\", \"\")\n",
    "                f.write(f\"{ticker},{name},{price},{exchange},{currency},{updated}\\n\")\n",
    "                print(f\"[stockprice] {s} → price {price}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[stockprice] {s}: ERROR → {e}\")\n",
    "\n",
    "def fetch_marketcap(symbols=NSE_TICKERS):\n",
    "    \"\"\"Fetch market cap using /v1/marketcap.\"\"\"\n",
    "    out_path = os.path.join(DATA_DIR, \"marketcap.csv\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ticker,name,market_cap,updated\\n\")\n",
    "        for s in symbols:\n",
    "            url = f\"https://api.api-ninjas.com/v1/marketcap?ticker={s}\"\n",
    "            try:\n",
    "                r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "                if r.status_code != 200:\n",
    "                    print(f\"[marketcap] {s}: status {r.status_code}\")\n",
    "                    continue\n",
    "                data = r.json()\n",
    "                ticker = data.get(\"ticker\", \"\")\n",
    "                name = data.get(\"name\", \"\")\n",
    "                mc = data.get(\"market_cap\", \"\")\n",
    "                updated = data.get(\"updated\", \"\")\n",
    "                f.write(f\"{ticker},{name},{mc},{updated}\\n\")\n",
    "                print(f\"[marketcap] {s} → market cap {mc}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[marketcap] {s}: ERROR → {e}\")\n",
    "\n",
    "def fetch_crypto(symbols=[\"BTCUSD\",\"ETHUSD\",\"DOGEUSD\"]):\n",
    "    \"\"\"Fetch crypto prices using /v1/cryptoprice.\"\"\"\n",
    "    out_path = os.path.join(DATA_DIR, \"cryptoprice.csv\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"symbol,price,timestamp\\n\")\n",
    "        for sym in symbols:\n",
    "            url = f\"https://api.api-ninjas.com/v1/cryptoprice?symbol={sym}\"\n",
    "            try:\n",
    "                r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "                if r.status_code != 200:\n",
    "                    print(f\"[cryptoprice] {sym}: status {r.status_code}\")\n",
    "                    continue\n",
    "                data = r.json()\n",
    "                price = data.get(\"price\", \"\")\n",
    "                ts = data.get(\"timestamp\", \"\")\n",
    "                f.write(f\"{sym},{price},{ts}\\n\")\n",
    "                print(f\"[cryptoprice] {sym} → {price}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[cryptoprice] {sym}: ERROR → {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf548011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stockprice] INFY → price 16.96\n",
      "[stockprice] RELIANCE: ERROR → 'list' object has no attribute 'get'\n",
      "[stockprice] HDFCBANK: ERROR → 'list' object has no attribute 'get'\n",
      "[stockprice] TCS → price 2.65\n",
      "[stockprice] ICICIBANK: ERROR → 'list' object has no attribute 'get'\n",
      "[marketcap] INFY → market cap 70304118129\n",
      "[marketcap] RELIANCE: ERROR → 'list' object has no attribute 'get'\n",
      "[marketcap] HDFCBANK: ERROR → 'list' object has no attribute 'get'\n",
      "[marketcap] TCS → market cap 9145813\n",
      "[marketcap] ICICIBANK: ERROR → 'list' object has no attribute 'get'\n",
      "[cryptoprice] BTCUSD → 115398.52000000\n",
      "[cryptoprice] ETHUSD → 4640.00000000\n",
      "[cryptoprice] DOGEUSD → 0.28564000\n"
     ]
    }
   ],
   "source": [
    "fetch_stockprice()\n",
    "fetch_marketcap()\n",
    "fetch_crypto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08c81b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Macro: CPI via data.gov.in --------\n",
    "def fetch_cpi():\n",
    "    if not DATA_GOV_IN_KEY:\n",
    "        print(\"[macro:cpi] Missing DATA_GOV_IN_KEY; skipping.\")\n",
    "        return\n",
    "\n",
    "    # This is the resource ID from your notes; adjust if you switch datasets\n",
    "    resource_id = \"352b3616-9d3d-42e5-80af-7d21a2a53fab\"\n",
    "    base = \"https://api.data.gov.in/resource/{rid}\"\n",
    "    limit = 100\n",
    "    offset = 0\n",
    "    out_path = os.path.join(MACRO_DIR, \"cpi.jsonl\")\n",
    "    old = read_jsonl(out_path)\n",
    "    # Use (financial_year, cpi_c_inflation_) as dedup keys OR (date if present)\n",
    "    seen_keys = set()\n",
    "    for o in old:\n",
    "        key = (o.get(\"financial_year\"), o.get(\"cpi_c_inflation_\"))\n",
    "        seen_keys.add(key)\n",
    "\n",
    "    appended = 0\n",
    "    while True:\n",
    "        params = {\n",
    "            \"api-key\": DATA_GOV_IN_KEY,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.get(base.format(rid=resource_id), params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            payload = resp.json()\n",
    "            rows = payload.get(\"records\", []) or payload.get(\"data\", [])\n",
    "        except Exception as e:\n",
    "            print(f\"[macro:cpi] Error at offset {offset}: {e}\")\n",
    "            break\n",
    "\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        batch = []\n",
    "        for r in rows:\n",
    "            # Normalize field names defensively\n",
    "            fy = r.get(\"financial_year\") or r.get(\"financial_year_\")\n",
    "            infl = r.get(\"cpi_c_inflation_\") or r.get(\"cpi_c_inflation\")\n",
    "            rec = {\n",
    "                \"financial_year\": fy,\n",
    "                \"cpi_c_inflation_\": try_float(infl),\n",
    "                \"source\": \"data.gov.in\",\n",
    "                \"country\": \"India\",\n",
    "                \"fetched_at\": datetime.utcnow().isoformat() + \"Z\"\n",
    "            }\n",
    "            key = (rec[\"financial_year\"], rec[\"cpi_c_inflation_\"])\n",
    "            if key not in seen_keys:\n",
    "                seen_keys.add(key)\n",
    "                batch.append(rec)\n",
    "\n",
    "        if batch:\n",
    "            append_jsonl(batch, out_path)\n",
    "            appended += len(batch)\n",
    "\n",
    "        offset += limit\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(f\"[macro:cpi] Appended {appended} new rows. Done.\")\n",
    "\n",
    "def try_float(x):\n",
    "    try:\n",
    "        return float(str(x).replace(\",\", \"\").strip())\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aa551f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cpi_monthly():\n",
    "    if not DATA_GOV_IN_KEY:\n",
    "        print(\"[macro:cpi] Missing DATA_GOV_IN_KEY\")\n",
    "        return\n",
    "    \n",
    "    # Monthly CPI dataset (Combined, Rural, Urban)\n",
    "    resource_id = \"9a10e07c-79f4-4db4-8c7f-fb6e4d9b5f49\"\n",
    "    base_url = f\"https://api.data.gov.in/resource/{resource_id}\"\n",
    "    \n",
    "    out_path = os.path.join(MACRO_DIR, \"cpi_monthly.csv\")\n",
    "    rows_all = []\n",
    "    offset, limit = 0, 100\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"api-key\": DATA_GOV_IN_KEY,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset\n",
    "        }\n",
    "        try:\n",
    "            r = requests.get(base_url, params=params, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            payload = r.json()\n",
    "            print(payload)\n",
    "            rows = payload.get(\"records\") or []\n",
    "        except Exception as e:\n",
    "            print(\"[macro:cpi] ERROR →\", e)\n",
    "            break\n",
    "        \n",
    "        if not rows:\n",
    "            break\n",
    "        \n",
    "        for row in rows:\n",
    "            # row contains: year, month, cpi_combined, cpi_rural, cpi_urban, etc.\n",
    "            year = row.get(\"year\")\n",
    "            month = row.get(\"month\")\n",
    "            combined = row.get(\"cpi_combined\")\n",
    "            rural = row.get(\"cpi_rural\")\n",
    "            urban = row.get(\"cpi_urban\")\n",
    "            rows_all.append((year, month, combined, rural, urban))\n",
    "        \n",
    "        offset += limit\n",
    "    \n",
    "    if rows_all:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"year,month,cpi_combined,cpi_rural,cpi_urban\\n\")\n",
    "            for y, m, c, r, u in rows_all:\n",
    "                f.write(f\"{y},{m},{c},{r},{u}\\n\")\n",
    "        print(f\"[macro:cpi] Saved {len(rows_all)} rows → {out_path}\")\n",
    "    else:\n",
    "        print(\"[macro:cpi] No data fetched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda6a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_34420\\408802813.py:50: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"fetched_at\": datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[macro:cpi] Appended 6 new rows. Done.\n"
     ]
    }
   ],
   "source": [
    "fetch_cpi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74eb948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Meta not found', 'version': '2.2.0', 'status': 'error', 'total': 0, 'count': 0, 'limit': '100', 'offset': '0', 'field': [], 'records': []}\n",
      "[macro:cpi] No data fetched\n"
     ]
    }
   ],
   "source": [
    "fetch_cpi_monthly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037d28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "OUT_CSV = os.path.join(NEWS_DIR, \"google_news.csv\")\n",
    "\n",
    "def build_google_news_rss(query: str, hl=\"en-IN\", gl=\"IN\", ceid=\"IN:en\") -> str:\n",
    "    \"\"\"\n",
    "    Build a Google News RSS search URL for a given query.\n",
    "    Example: query=\"RBI repo rate\" → Google News headlines focused on India.\n",
    "    \"\"\"\n",
    "    from urllib.parse import quote_plus\n",
    "    q = quote_plus(query)\n",
    "    return f\"https://news.google.com/rss/search?q={q}&hl={hl}&gl={gl}&ceid={ceid}\"\n",
    "\n",
    "def _read_existing_links(path: str) -> set:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    seen = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            link = (row.get(\"link\") or \"\").strip()\n",
    "            if link:\n",
    "                seen.add(link)\n",
    "    return seen\n",
    "\n",
    "def _ts_to_iso(entry) -> str:\n",
    "    # feedparser often provides published_parsed (time.struct_time)\n",
    "    ts = entry.get(\"published_parsed\") or entry.get(\"updated_parsed\")\n",
    "    if ts:\n",
    "        return datetime(*ts[:6]).isoformat()\n",
    "    # fallback to raw published string\n",
    "    return (entry.get(\"published\") or entry.get(\"updated\") or \"\").strip()\n",
    "\n",
    "def fetch_google_news(\n",
    "    queries,\n",
    "    per_query_limit: int = 50,\n",
    "    sleep_between: float = 0.3,\n",
    "    out_csv: str = OUT_CSV\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch Google News RSS for a list of queries and append to CSV.\n",
    "    Dedup strictly by 'link'.\n",
    "    \"\"\"\n",
    "    # Prepare CSV (create with header if not exists)\n",
    "    file_exists = os.path.exists(out_csv)\n",
    "    if not file_exists:\n",
    "        with open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"title\", \"link\", \"source_feed\", \"published\", \"fetched_at\", \"query\"])\n",
    "\n",
    "    seen_links = _read_existing_links(out_csv)\n",
    "    added = 0\n",
    "\n",
    "    with open(out_csv, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for q in queries:\n",
    "            feed_url = build_google_news_rss(q)\n",
    "            feed = feedparser.parse(feed_url)\n",
    "            entries = feed.entries[:per_query_limit] if getattr(feed, \"entries\", None) else []\n",
    "\n",
    "            for e in entries:\n",
    "                title = (e.get(\"title\") or \"\").strip()\n",
    "                link = (e.get(\"link\") or \"\").strip()\n",
    "                if not link or link in seen_links:\n",
    "                    continue\n",
    "\n",
    "                published_iso = _ts_to_iso(e)\n",
    "                fetched_at = datetime.now().isoformat()\n",
    "                writer.writerow([title, link, feed_url, published_iso, fetched_at, q])\n",
    "                seen_links.add(link)\n",
    "                added += 1\n",
    "\n",
    "            time.sleep(sleep_between)  # be polite\n",
    "\n",
    "    print(f\"[news] Added {added} new rows → {out_csv}\")\n",
    "\n",
    "# ---- Example usage ----\n",
    "QUERIES = [\n",
    "    \"RBI repo rate\",\n",
    "    \"NIFTY 50\",\n",
    "    \"Reliance Industries\",\n",
    "    \"HDFC Bank\",\n",
    "    \"Infosys\",\n",
    "    \"India CPI inflation\",\n",
    "    \"RBI MPC meeting\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "533b0b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[news] Added 337 new rows → C:\\Users\\harsh\\OneDrive\\Desktop\\LLM Capstone\\Data Collection\\data\\news\\google_news.csv\n"
     ]
    }
   ],
   "source": [
    "fetch_google_news(QUERIES, per_query_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4734592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Robust GDELT v2 Doc API fetcher (handles OR syntax, optional weekly slices) ----\n",
    "import os, csv, time, requests, pandas as pd, re\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "NEWS_DIR = os.path.join(os.getcwd(), \"data\", \"news_gdelt\")\n",
    "os.makedirs(NEWS_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(NEWS_DIR, \"gdelt_news.csv\")\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (capstone-news-bot/1.0)\"}\n",
    "\n",
    "def ensure_header(path, header):\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(header)\n",
    "\n",
    "def read_seen_urls(path):\n",
    "    if not os.path.exists(path): return set()\n",
    "    seen = set()\n",
    "    try:\n",
    "        for chunk in pd.read_csv(path, usecols=[\"url\"], chunksize=50000):\n",
    "            seen.update(chunk[\"url\"].dropna().astype(str).tolist())\n",
    "    except Exception:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            for row in csv.DictReader(f):\n",
    "                u = (row.get(\"url\") or \"\").strip()\n",
    "                if u: seen.add(u)\n",
    "    return seen\n",
    "\n",
    "def month_windows(start_d: date, end_d: date):\n",
    "    cur = date(start_d.year, start_d.month, 1)\n",
    "    while cur <= end_d:\n",
    "        nxt = (cur + relativedelta(months=1)) - relativedelta(days=1)\n",
    "        s = max(cur, start_d); e = min(nxt, end_d)\n",
    "        yield s, e\n",
    "        cur = cur + relativedelta(months=1)\n",
    "\n",
    "def week_windows(start_d: date, end_d: date):\n",
    "    s = start_d\n",
    "    while s <= end_d:\n",
    "        e = min(s + timedelta(days=6), end_d)\n",
    "        yield s, e\n",
    "        s = e + timedelta(days=1)\n",
    "\n",
    "def dtstr(d: date, end=False):\n",
    "    return f\"{d.strftime('%Y%m%d')}{'235959' if end else '000000'}\"\n",
    "\n",
    "def sanitize_query(q: str) -> str:\n",
    "    qs = q.strip()\n",
    "\n",
    "    # 1) wrap OR groups if not already wrapped\n",
    "    if \" OR \" in qs and \"(\" not in qs:\n",
    "        qs = f\"({qs})\"\n",
    "\n",
    "    # 2) unquote any too-short phrase (<=3 chars) to satisfy GDELT\n",
    "    #    e.g.  \"RBI\" -> RBI,  \"GST\" -> GST\n",
    "    def _unquote_short(m):\n",
    "        inner = m.group(1)\n",
    "        return inner if len(inner) <= 3 else f\"\\\"{inner}\\\"\"\n",
    "    qs = re.sub(r'\"([^\"]+)\"', _unquote_short, qs)\n",
    "\n",
    "    # 3) add useful filters (drop if you want max breadth)\n",
    "    if \"sourcelang:\" not in qs:\n",
    "        qs += \" sourcelang:English\"\n",
    "    if \"sourcecountry:\" not in qs:\n",
    "        qs += \" sourcecountry:IN\"\n",
    "\n",
    "    return qs\n",
    "\n",
    "def fetch_gdelt_doc(query: str, start_d: date, end_d: date, maxrecords=250, timeout=30):\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"mode\": \"ArtList\",\n",
    "        \"startdatetime\": dtstr(start_d, False),\n",
    "        \"enddatetime\": dtstr(end_d, True),\n",
    "        \"maxrecords\": str(maxrecords),\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)\n",
    "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"json\" not in ctype:\n",
    "            # likely an error page; show a short hint then skip\n",
    "            print(f\"[gdelt] non-JSON ({r.status_code}) for {start_d}..{end_d}: {r.text[:90].replace(chr(10),' ')}\")\n",
    "            return []\n",
    "        data = r.json()\n",
    "        return data.get(\"articles\", []) or []\n",
    "    except Exception as e:\n",
    "        print(f\"[gdelt] EXC {start_d}..{end_d}: {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_gdelt_bulk(\n",
    "    queries,\n",
    "    start=\"2023-01-01\",\n",
    "    end=None,\n",
    "    per_window_cap=250,\n",
    "    use_weekly_slices=False,\n",
    "    sleep_s=0.5,\n",
    "):\n",
    "    if end is None: end = date.today().isoformat()\n",
    "    start_d = datetime.fromisoformat(start).date()\n",
    "    end_d   = datetime.fromisoformat(end).date()\n",
    "\n",
    "    header = [\"query\",\"title\",\"seendate\",\"url\",\"domain\",\"language\",\"country\",\"source\",\"socialimage\"]\n",
    "    ensure_header(OUT_CSV, header)\n",
    "    seen = read_seen_urls(OUT_CSV)\n",
    "\n",
    "    total_added = 0\n",
    "    with open(OUT_CSV, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        for q in queries:\n",
    "            q_sane = sanitize_query(q)\n",
    "            print(f\"[gdelt] Query: {q_sane}\")\n",
    "            windows = week_windows(start_d, end_d) if use_weekly_slices else month_windows(start_d, end_d)\n",
    "            for s_dt, e_dt in windows:\n",
    "                # basic retry with backoff\n",
    "                arts = []\n",
    "                for attempt in range(3):\n",
    "                    arts = fetch_gdelt_doc(q_sane, s_dt, e_dt, maxrecords=per_window_cap)\n",
    "                    if arts: break\n",
    "                    time.sleep(0.8 * (attempt + 1))\n",
    "                added_w = 0\n",
    "                for a in arts:\n",
    "                    url = (a.get(\"url\") or \"\").strip()\n",
    "                    if not url or url in seen: continue\n",
    "                    wr.writerow([\n",
    "                        q_sane,\n",
    "                        (a.get(\"title\") or \"\").replace(\"\\n\",\" \").strip(),\n",
    "                        a.get(\"seendate\",\"\"),\n",
    "                        url,\n",
    "                        a.get(\"domain\",\"\"),\n",
    "                        a.get(\"language\",\"\"),\n",
    "                        a.get(\"country\",\"\"),\n",
    "                        a.get(\"source\",\"\"),\n",
    "                        a.get(\"socialimage\",\"\"),\n",
    "                    ])\n",
    "                    seen.add(url)\n",
    "                    total_added += 1\n",
    "                    added_w += 1\n",
    "                time.sleep(sleep_s)\n",
    "            print(f\"[gdelt]   cumulative added this run: {total_added}\")\n",
    "    print(f\"[gdelt] DONE. Added {total_added} new rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2084259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gdelt] Query: (RBI OR \"Reserve Bank of India\") sourcelang:English sourcecountry:IN\n",
      "[gdelt]   cumulative added this run: 6538\n",
      "[gdelt] Query: (\"CPI inflation\" OR \"consumer price index\") AND India sourcelang:English sourcecountry:IN\n",
      "[gdelt]   cumulative added this run: 7019\n",
      "[gdelt] Query: (\"NIFTY 50\" OR Sensex) sourcelang:English sourcecountry:IN\n",
      "[gdelt]   cumulative added this run: 13038\n",
      "[gdelt] Query: (\"HDFC Bank\" OR \"ICICI Bank\" OR SBI) sourcelang:English sourcecountry:IN\n",
      "[gdelt]   cumulative added this run: 17135\n",
      "[gdelt] Query: (\"Reliance Industries\" OR RIL) sourcelang:English sourcecountry:IN\n",
      "[gdelt]   cumulative added this run: 18695\n",
      "[gdelt] Query: (\"Infosys\" OR TCS OR Wipro) sourcelang:English sourcecountry:IN\n",
      "[gdelt]   cumulative added this run: 21894\n",
      "[gdelt] DONE. Added 21894 new rows → c:\\Users\\harsh\\OneDrive\\Desktop\\LLM Capstone\\Data Collection\\data\\news_gdelt\\gdelt_news.csv\n"
     ]
    }
   ],
   "source": [
    "QUERIES = [\n",
    "    '(RBI OR \"Reserve Bank of India\")',   # RBI unquoted now\n",
    "    '(\"CPI inflation\" OR \"consumer price index\") AND India',\n",
    "    '(\"NIFTY 50\" OR Sensex)',             # Sensex can be unquoted\n",
    "    '(\"HDFC Bank\" OR \"ICICI Bank\" OR SBI)',\n",
    "    '(\"Reliance Industries\" OR RIL)',\n",
    "    '(\"Infosys\" OR TCS OR Wipro)',\n",
    "]\n",
    "\n",
    "fetch_gdelt_bulk(\n",
    "    QUERIES,\n",
    "    start=\"2025-03-01\",\n",
    "    end=\"2025-09-14\",\n",
    "    per_window_cap=250,\n",
    "    use_weekly_slices=True,\n",
    "    sleep_s=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8058225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, csv, random\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import robotparser\n",
    "\n",
    "import trafilatura\n",
    "from trafilatura.settings import use_config\n",
    "from readability import Document\n",
    "\n",
    "# ---------- paths\n",
    "BASE = os.getcwd()\n",
    "GDELT_CSV = os.path.join(BASE, \"data\", \"news_gdelt\", \"gdelt_news.csv\")\n",
    "ART_DIR   = os.path.join(BASE, \"data\", \"news_articles\")\n",
    "ART_JSONL = os.path.join(ART_DIR, \"articles.jsonl\")\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- config\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/124.0.0.0 Safari/537.36\"\n",
    ")\n",
    "HEADERS = {\n",
    "    \"User-Agent\": USER_AGENT,\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-IN,en;q=0.9\",\n",
    "    \"Referer\": \"https://news.google.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "SLEEP_BASE = 0.8      # base delay between requests\n",
    "SLEEP_JITTER = 0.5    # add a little randomness\n",
    "TIMEOUT = 25\n",
    "\n",
    "# keep this small to start; raise later\n",
    "MAX_PAGES = 500\n",
    "\n",
    "# Allow a focused set of Indian finance/major outlets (expand as needed)\n",
    "ALLOW_HOST_SUBSTRINGS = [\n",
    "    \"reuters.com\", \"economictimes.indiatimes.com\", \"moneycontrol.com\", \"livemint.com\",\n",
    "    \"business-standard.com\", \"thehindubusinessline.com\", \"financialexpress.com\",\n",
    "    \"indiatimes.com\", \"cnbctv18.com\", \"thehindu.com\", \"ndtv.com\", \"hindustantimes.com\",\n",
    "    \"timesofindia.indiatimes.com\", \"news18.com\", \"mintgenie.livemint.com\",\n",
    "]\n",
    "\n",
    "# ---------- robots handling (cache)\n",
    "robots_cache = {}\n",
    "\n",
    "def can_fetch(url: str, user_agent: str = USER_AGENT) -> bool:\n",
    "    \"\"\"Respect robots.txt; if robots unreadable, default to False (skip).\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        rp = robots_cache.get(base)\n",
    "        if rp is None:\n",
    "            rp = robotparser.RobotFileParser()\n",
    "            rp.set_url(f\"{base}/robots.txt\")\n",
    "            try:\n",
    "                rp.read()\n",
    "            except Exception:\n",
    "                robots_cache[base] = rp\n",
    "                return False\n",
    "            robots_cache[base] = rp\n",
    "        return rp.can_fetch(user_agent, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def host_allowed(url: str) -> bool:\n",
    "    host = urlparse(url).netloc.lower()\n",
    "    return any(dom in host for dom in ALLOW_HOST_SUBSTRINGS)\n",
    "\n",
    "# ---------- load seen URLs from jsonl to make runs resumable\n",
    "def load_seen_urls(jsonl_path: str) -> set:\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        return set()\n",
    "    seen = set()\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                u = (obj.get(\"url\") or \"\").strip()\n",
    "                if u:\n",
    "                    seen.add(u)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return seen\n",
    "\n",
    "# ---------- requests session\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "# ---------- trafilatura config (parse HTML we already fetched)\n",
    "t_cfg = use_config()\n",
    "t_cfg.set(\"DEFAULT\", \"user_agent\", USER_AGENT)\n",
    "t_cfg.set(\"DEFAULT\", \"favor_reliable_domains\", \"true\")\n",
    "t_cfg.set(\"DEFAULT\", \"include_comments\", \"false\")\n",
    "t_cfg.set(\"DEFAULT\", \"no_fallback\", \"false\")   # allow internal fallbacks\n",
    "\n",
    "def extract_with_trafilatura(html: str, base_url: str) -> str | None:\n",
    "    \"\"\"Return plain text using trafilatura from HTML string.\"\"\"\n",
    "    if not html:\n",
    "        return None\n",
    "    txt = trafilatura.extract(html, config=t_cfg, url=base_url)\n",
    "    # print(txt)\n",
    "    if txt and len(txt.strip()) > 400:\n",
    "        return txt.strip()\n",
    "    # print(\"Nothing parsed\")\n",
    "    return None\n",
    "\n",
    "def extract_with_readability(html: str) -> str | None:\n",
    "    \"\"\"Fallback: readability-lxml content extraction.\"\"\"\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        article_html = doc.summary(html_partial=True)\n",
    "        soup = BeautifulSoup(article_html, \"html.parser\")\n",
    "        text = \" \".join(p.get_text(\" \", strip=True) for p in soup.find_all([\"p\", \"li\"]))\n",
    "        text = \" \".join(text.split())\n",
    "        if len(text) > 400:\n",
    "            return text\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def find_amp_link(html: str, url: str) -> str | None:\n",
    "    \"\"\"Look for <link rel='amphtml'> or a common AMP variant of the URL.\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        link = soup.find(\"link\", rel=lambda v: v and \"amphtml\" in v.lower())\n",
    "        if link and link.get(\"href\"):\n",
    "            amp = link[\"href\"]\n",
    "            # make absolute if relative\n",
    "            if amp.startswith(\"//\"):\n",
    "                amp = urlparse(url).scheme + \":\" + amp\n",
    "            elif amp.startswith(\"/\"):\n",
    "                parsed = urlparse(url)\n",
    "                amp = f\"{parsed.scheme}://{parsed.netloc}{amp}\"\n",
    "            return amp\n",
    "        # heuristic fallbacks: /amp or ?outputType=amp\n",
    "        if url.endswith(\"/\"):\n",
    "            return url + \"amp\"\n",
    "        return url + (\"/amp\" if not url.endswith(\"/amp\") else \"\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_html(url: str) -> str | None:\n",
    "    try:\n",
    "        r = session.get(url, timeout=TIMEOUT, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"text/html\" not in ctype:\n",
    "            return None\n",
    "        return r.text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def scrape_from_gdelt(\n",
    "    gdelt_csv: str = GDELT_CSV,\n",
    "    out_jsonl: str = ART_JSONL,\n",
    "    max_pages: int = MAX_PAGES,\n",
    "    base_sleep: float = SLEEP_BASE,\n",
    "    jitter: float = SLEEP_JITTER\n",
    "):\n",
    "    if not os.path.exists(gdelt_csv):\n",
    "        print(\"[scrape] GDELT CSV not found:\", gdelt_csv)\n",
    "        return\n",
    "\n",
    "    # Load GDELT rows (url, title, seendate, domain, query)\n",
    "    try:\n",
    "        df = pd.read_csv(gdelt_csv, usecols=[\"url\",\"title\",\"seendate\",\"domain\",\"query\"])\n",
    "    except Exception:\n",
    "        df = pd.read_csv(gdelt_csv)\n",
    "\n",
    "    df = df.dropna(subset=[\"url\"]).drop_duplicates(subset=[\"url\"])\n",
    "    seen = load_seen_urls(out_jsonl)\n",
    "\n",
    "    added = 0\n",
    "    with open(out_jsonl, \"a\", encoding=\"utf-8\") as out:\n",
    "        for _, row in df.iterrows():\n",
    "            url   = str(row.get(\"url\") or \"\").strip()\n",
    "            title = str(row.get(\"title\") or \"\").strip()\n",
    "            if not url or url in seen:\n",
    "                continue\n",
    "            if not host_allowed(url):\n",
    "                continue\n",
    "            if not can_fetch(url):\n",
    "                continue\n",
    "\n",
    "            # 1) fetch original HTML\n",
    "            html = get_html(url)\n",
    "            # 2) try trafilatura on original\n",
    "            text = extract_with_trafilatura(html, url) if html else None\n",
    "            # print(text)\n",
    "            # 3) try AMP if original failed/short\n",
    "            if not text:\n",
    "                amp_url = find_amp_link(html or \"\", url)\n",
    "                if amp_url and amp_url != url and can_fetch(amp_url):\n",
    "                    amp_html = get_html(amp_url)\n",
    "                    text = extract_with_trafilatura(amp_html, amp_url) if amp_html else None\n",
    "\n",
    "            # 4) readability fallback\n",
    "            if not text and html:\n",
    "                text = extract_with_readability(html)\n",
    "\n",
    "            if text:\n",
    "                rec = {\n",
    "                    \"url\": url,\n",
    "                    \"title\": title,\n",
    "                    \"seendate\": str(row.get(\"seendate\") or \"\"),\n",
    "                    \"domain\": str(row.get(\"domain\") or \"\"),\n",
    "                    \"query\": str(row.get(\"query\") or \"\"),\n",
    "                    \"fetched_at\": datetime.now().isoformat(),\n",
    "                    \"text\": text,\n",
    "                    \"text_len\": len(text)\n",
    "                }\n",
    "                out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                seen.add(url)\n",
    "                added += 1\n",
    "\n",
    "            # polite sleep with jitter\n",
    "            time.sleep(base_sleep + random.random() * jitter)\n",
    "\n",
    "            if added >= max_pages:\n",
    "                break\n",
    "\n",
    "    print(f\"[scrape] Added {added} articles → {out_jsonl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b373d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scrape] Added 300 articles → c:\\Users\\harsh\\OneDrive\\Desktop\\LLM Capstone\\Data Collection\\data\\news_articles\\articles.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ---- Run a small batch first; then increase MAX_PAGES and re-run\n",
    "scrape_from_gdelt(max_pages=300, base_sleep=0.9, jitter=0.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
