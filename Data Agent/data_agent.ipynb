{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79dbb5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\OneDrive\\Desktop\\LLM Capstone\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss  # type: ignore\n",
    "_FAISS_OK = True\n",
    "\n",
    "\n",
    "class RetrieverIndex:\n",
    "    \"\"\"\n",
    "    Loads a FAISS index if possible; otherwise loads/creates a local NumPy index:\n",
    "      - uses documents.parquet and metadata.parquet\n",
    "      - builds/saves embeddings.npz on first run if not present\n",
    "    \"\"\"\n",
    "    def __init__(self, store_dir: Path, embed_model_name: str):\n",
    "        self.store_dir = Path(store_dir)\n",
    "        self.embedder = SentenceTransformer(embed_model_name, device=\"cpu\")\n",
    "\n",
    "        self.index_faiss = self.store_dir / \"index.faiss\"\n",
    "        self.meta_path   = self.store_dir / \"metadata.parquet\"\n",
    "        self.docs_path   = self.store_dir / \"documents.parquet\"\n",
    "        self.emb_npz     = self.store_dir / \"embeddings.npz\"  # optional\n",
    "\n",
    "        if not self.meta_path.exists() or not self.docs_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Missing metadata/documents parquet in: {self.store_dir}\\n\"\n",
    "                f\"Expected files: {self.meta_path.name}, {self.docs_path.name}\"\n",
    "            )\n",
    "\n",
    "        self.meta = pd.read_parquet(self.meta_path)\n",
    "        self.docs = pd.read_parquet(self.docs_path)\n",
    "\n",
    "        # Choose backend\n",
    "        self.backend = \"faiss\" if _FAISS_OK and self.index_faiss.exists() else \"numpy\"\n",
    "\n",
    "        if self.backend == \"faiss\":\n",
    "            self.index = faiss.read_index(str(self.index_faiss))\n",
    "            # Ensure ids column exists in meta/docs\n",
    "            if \"id\" not in self.meta.columns or \"id\" not in self.docs.columns:\n",
    "                raise ValueError(\"Parquet metadata/documents must contain an 'id' column aligned to FAISS IDs.\")\n",
    "        else:\n",
    "            # NumPy fallback: we need embeddings matrix\n",
    "            if self.emb_npz.exists():\n",
    "                self.emb = np.load(self.emb_npz)[\"emb\"].astype(np.float32)\n",
    "            else:\n",
    "                # Build embeddings once, save to disk\n",
    "                texts = self.docs.sort_values(\"id\")[\"text\"].tolist()\n",
    "                embs: List[np.ndarray] = []\n",
    "                B = 512\n",
    "                for i in range(0, len(texts), B):\n",
    "                    batch = self.embedder.encode(\n",
    "                        texts[i:i+B],\n",
    "                        batch_size=64,\n",
    "                        normalize_embeddings=True,\n",
    "                        convert_to_numpy=True,\n",
    "                        show_progress_bar=False,\n",
    "                    )\n",
    "                    embs.append(batch.astype(np.float32))\n",
    "                self.emb = np.vstack(embs)\n",
    "                np.savez_compressed(self.emb_npz, emb=self.emb)\n",
    "\n",
    "            # Align meta/docs by id ascending\n",
    "            self.meta = self.meta.sort_values(\"id\").reset_index(drop=True)\n",
    "            self.docs = self.docs.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "    def search(self, query: str, k: int = 10) -> List[Dict[str, Any]]:\n",
    "        q = self.embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True).astype(np.float32)\n",
    "        if self.backend == \"faiss\":\n",
    "            sims, ids = self.index.search(q, k)\n",
    "            sims, ids = sims[0], ids[0]\n",
    "            rows = []\n",
    "            id_to_row = self.meta.set_index(\"id\")\n",
    "            txt_map = self.docs.set_index(\"id\")[\"text\"]\n",
    "            for s, i in zip(sims, ids):\n",
    "                md = id_to_row.loc[int(i)].to_dict()\n",
    "                md[\"score\"] = float(s)\n",
    "                md[\"text\"] = txt_map.loc[int(i)][:240]\n",
    "                rows.append(md)\n",
    "            return rows\n",
    "        else:\n",
    "            # cosine with normalized vectors => inner product\n",
    "            sims = (self.emb @ q[0])\n",
    "            idx = np.argpartition(-sims, k)[:k]\n",
    "            top = idx[np.argsort(-sims[idx])]\n",
    "            rows = []\n",
    "            for i in top:\n",
    "                md = self.meta.iloc[int(i)].to_dict()\n",
    "                md[\"score\"] = float(sims[i])\n",
    "                md[\"text\"] = self.docs.iloc[int(i)][\"text\"][:240]\n",
    "                rows.append(md)\n",
    "            return rows\n",
    "\n",
    "    def info(self) -> str:\n",
    "        n = len(self.meta)\n",
    "        return f\"RetrieverIndex(backend={self.backend}, vectors={n})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1603e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import yfinance as yf\n",
    "import feedparser\n",
    "\n",
    "from config import VECTOR_FAISS_DIR, EMBEDDING_MODEL, TOP_K\n",
    "\n",
    "class DataAgent:\n",
    "    def __init__(self):\n",
    "        self.index = RetrieverIndex(VECTOR_FAISS_DIR, EMBEDDING_MODEL)\n",
    "\n",
    "    # ---------- Vector retrieval ----------\n",
    "    def retrieve(self, query: str, k: int = TOP_K) -> List[Dict[str, Any]]:\n",
    "        return self.index.search(query, k=k)\n",
    "\n",
    "    # ---------- Live prices ----------\n",
    "    def fetch_prices(self, symbols: List[str], period: str = \"1mo\", interval: str = \"1d\") -> Dict[str, List[Dict[str, Any]]]:\n",
    "        out: Dict[str, List[Dict[str, Any]]] = {}\n",
    "        for s in symbols:\n",
    "            try:\n",
    "                df = yf.Ticker(s).history(period=period, interval=interval, auto_adjust=False)\n",
    "                if df.empty: continue\n",
    "                df = df.rename_axis(\"date\").reset_index()\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.tz_localize(None)\n",
    "                df = df.rename(columns={\n",
    "                    \"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\"Close\":\"close\",\n",
    "                    \"Adj Close\":\"adj_close\",\"Volume\":\"volume\"\n",
    "                })\n",
    "                if \"adj_close\" not in df.columns and \"close\" in df.columns:\n",
    "                    df[\"adj_close\"] = df[\"close\"]\n",
    "                out[s] = df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"]].tail(5).to_dict(orient=\"records\")\n",
    "                time.sleep(0.15)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return out\n",
    "\n",
    "    # ---------- Google News RSS ----------\n",
    "    @staticmethod\n",
    "    def _rss_url(q: str, hl=\"en-IN\", gl=\"IN\", ceid=\"IN:en\") -> str:\n",
    "        from urllib.parse import quote_plus\n",
    "        return f\"https://news.google.com/rss/search?q={quote_plus(q)}&hl={hl}&gl={gl}&ceid={ceid}\"\n",
    "\n",
    "    def fetch_rss(self, queries: List[str], per_query_limit: int = 30, sleep_s: float = 0.25) -> List[Dict[str, Any]]:\n",
    "        items, seen = [], set()\n",
    "        for q in queries:\n",
    "            feed_url = self._rss_url(q)\n",
    "            feed = feedparser.parse(feed_url)\n",
    "            entries = feed.entries[:per_query_limit] if getattr(feed, \"entries\", None) else []\n",
    "            for e in entries:\n",
    "                link = (e.get(\"link\") or \"\").strip()\n",
    "                if not link or link in seen: continue\n",
    "                seen.add(link)\n",
    "                items.append({\n",
    "                    \"title\": (e.get(\"title\") or \"\").strip(),\n",
    "                    \"link\": link,\n",
    "                    \"published\": (e.get(\"published\") or e.get(\"updated\") or \"\").strip(),\n",
    "                    \"source_feed\": feed_url,\n",
    "                    \"query\": q,\n",
    "                })\n",
    "            time.sleep(sleep_s)\n",
    "        return items\n",
    "\n",
    "    # ---------- One-call pipeline ----------\n",
    "    def _latest_summary(self, df: pd.DataFrame) -> dict:\n",
    "        s = df.copy()\n",
    "        s[\"date\"] = pd.to_datetime(s[\"date\"]).dt.tz_localize(None)\n",
    "        s = s.sort_values(\"date\")\n",
    "        last = s.iloc[-1]\n",
    "        prev = s.iloc[-2] if len(s) > 1 else last\n",
    "        # realized vol over last 10 closes (simple annualization with sqrt(252))\n",
    "        closes = s[\"close\"].astype(float).tail(10).pct_change().dropna()\n",
    "        vol10 = float(closes.std() * (252 ** 0.5)) if len(closes) > 1 else None\n",
    "        pct = float((last[\"close\"] - prev[\"close\"]) / prev[\"close\"]) if prev[\"close\"] else 0.0\n",
    "        return {\n",
    "            \"as_of\": last[\"date\"].isoformat(),\n",
    "            \"latest_close\": float(last[\"close\"]),\n",
    "            \"pct_change_1d\": pct,\n",
    "            \"realized_vol_10d\": vol10,\n",
    "            \"vendor\": \"yfinance\"\n",
    "        }\n",
    "\n",
    "    def _trim_text(self, txt: str, max_tokens: int, tokenizer) -> str:\n",
    "        if not max_tokens: \n",
    "            return txt\n",
    "        ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "        if len(ids) <= max_tokens: \n",
    "            return txt\n",
    "        keep = tokenizer.decode(ids[:max_tokens], skip_special_tokens=True)\n",
    "        return keep\n",
    "\n",
    "    def run_pipeline(\n",
    "        self,\n",
    "        user_query: str,\n",
    "        tickers: List[str],\n",
    "        rss_queries: List[str],\n",
    "        k: int = TOP_K,\n",
    "        limit_tokens_for_evidence: int = 256,\n",
    "        max_latency_seconds: int = 8\n",
    "    ) -> Dict[str, Any]:\n",
    "        import time, math\n",
    "        from transformers import AutoTokenizer\n",
    "        t0 = time.time()\n",
    "        tok = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)\n",
    "\n",
    "        t1 = time.time()\n",
    "        evidence = self.retrieve(user_query, k=k)\n",
    "        # enrich and trim evidence\n",
    "        ev_out = []\n",
    "        for e in evidence:\n",
    "            ev_out.append({\n",
    "                \"id\": int(e.get(\"id\", e.get(\"chunk\", 0))) if isinstance(e, dict) else None,\n",
    "                \"external_id\": f\"{e.get('url','')}|{e.get('chunk',0)}\",\n",
    "                \"url\": e.get(\"url\",\"\"),\n",
    "                \"title\": e.get(\"title\",\"\"),\n",
    "                \"published\": e.get(\"published\",\"\"),\n",
    "                \"domain\": e.get(\"domain\",\"\"),\n",
    "                \"score\": float(e.get(\"score\", 0.0)),\n",
    "                \"chunk\": int(e.get(\"chunk\", 0)),\n",
    "                \"text\": self._trim_text(e.get(\"text\",\"\"), limit_tokens_for_evidence, tok)\n",
    "            })\n",
    "        t2 = time.time()\n",
    "        prices_raw = self.fetch_prices(tickers)\n",
    "        market = {\"symbols\": {}, \"timeseries\": {}}\n",
    "        for sym, rows in prices_raw.items():\n",
    "            df = pd.DataFrame(rows)\n",
    "            if not df.empty:\n",
    "                market[\"symbols\"][sym] = self._latest_summary(df)\n",
    "                # keep a small tail for context, already ISO in your fetcher\n",
    "                market[\"timeseries\"][sym] = df.tail(5).to_dict(orient=\"records\")\n",
    "        t3 = time.time()\n",
    "        headlines = self.fetch_rss(rss_queries)\n",
    "        t4 = time.time()\n",
    "\n",
    "        bundle = {\n",
    "            \"query\": {\n",
    "                \"text\": user_query,\n",
    "                \"timestamp\": pd.Timestamp.now(tz=\"Asia/Kolkata\").isoformat()\n",
    "            },\n",
    "            \"evidence\": ev_out,\n",
    "            \"market\": market,\n",
    "            \"news\": {\n",
    "                \"rss\": headlines,\n",
    "                \"source\": \"GoogleNewsRSS\"\n",
    "            },\n",
    "            \"diagnostics\": {\n",
    "                \"index_backend\": self.index.backend if hasattr(self, \"index\") else \"unknown\",\n",
    "                \"vectors\": len(getattr(self.index, \"meta\", [])) if hasattr(self.index, \"meta\") else None,\n",
    "                \"timing_ms\": {\n",
    "                    \"total\": int((t4 - t0) * 1000),\n",
    "                    \"retrieve\": int((t2 - t1) * 1000),\n",
    "                    \"prices\": int((t3 - t2) * 1000),\n",
    "                    \"rss\": int((t4 - t3) * 1000)\n",
    "                },\n",
    "                \"errors\": []\n",
    "            }\n",
    "        }\n",
    "        # optional persistence for reproducibility\n",
    "        try:\n",
    "            from pathlib import Path\n",
    "            import json, re\n",
    "            slug = re.sub(r\"[^a-z0-9]+\", \"-\", user_query.lower()).strip(\"-\")[:60]\n",
    "            runs_dir = Path(\"runs\"); runs_dir.mkdir(exist_ok=True)\n",
    "            out_path = runs_dir / f\"{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}_{slug}.json\"\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(bundle, f, ensure_ascii=False, indent=2)\n",
    "            bundle[\"diagnostics\"][\"persisted\"] = str(out_path)\n",
    "        except Exception as ex:\n",
    "            bundle[\"diagnostics\"][\"errors\"].append(f\"persist_fail: {ex}\")\n",
    "        return bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a04ca391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrieverIndex(backend=faiss, vectors=3544)\n",
      "\n",
      "=== Retrieval (top chunks) ===\n",
      "[1] score=0.7860 | infosys share - Infosys down 3 % today ; should you buy this stock ahead of Q4 results on April 17 ? - Infosys down 3 % today ; should you buy this stock ahead of Q4 results on April 17 ? BusinessToday\n",
      "https://www.businesstoday.in/markets/stocks/story/infosys-down-3-today-should-you-buy-this-stock-ahead-of-q4-results-on-april-17-470106-2025-04-01\n",
      "infosys share - Infosys down 3 % today ; should you buy this stock ahead of Q4 results on April 17 ? - Infosys down 3 % today ; should you buy this stock ahead of Q4 results on April 17 ? BusinessToday...\n",
      "\n",
      "[2] score=0.7503 | Infosys plunges 16 % in 2025 : 5 key factors investors need to know ahead of Q4 earnings\n",
      "https://www.financialexpress.com/shorts/market/infosys-plunges-16-in-2025-5-key-factors-investors-need-to-know-ahead-of-q4-earnings-3779157/\n",
      "Infosys plunges 16 % in 2025 : 5 key factors investors need to know ahead of Q4 earnings...\n",
      "\n",
      "[3] score=0.7171 | IT shares fall - Infosys , Wipro , LTIMindtree , TCS , Coforge , Mphasis , Zensar shares tumble up to 5 % today , here why  - Infosys , Wipro , LTIMindtree , TCS , Coforge , Mphasis , Zensar shares tumble up to 5 % today , here why  BusinessToday\n",
      "https://www.businesstoday.in/markets/stocks/story/infosys-wipro-ltimindtree-tcs-coforge-mphasis-zensar-shares-fall-trump-tariffs-467519-2025-03-11\n",
      "IT shares fall - Infosys , Wipro , LTIMindtree , TCS , Coforge , Mphasis , Zensar shares tumble up to 5 % today , here why - Infosys , Wipro , LTIMindtree , TCS , Coforge , Mphasis , Zensar shares tumble up to 5 % today ...\n",
      "\n",
      "[4] score=0.7169 | Double down on India , but dont overlook Vietnam a hidden investment gem , says Jim Walker\n",
      "https://economictimes.indiatimes.com/markets/stocks/news/double-down-on-india-but-dont-overlook-vietnama-hidden-investment-gem-says-jim-walker/articleshow/118914696.cms\n",
      "i think that is still one of the best bets. ” ( disclaimer : recommendations, suggestions, views, and opinions given by experts are their own. These do not represent the views of the Economic Times) (What's moving Sensex...\n",
      "\n",
      "[5] score=0.7152 | Why retail investors should hold cash in volatile markets , Pankaj Pandey explains\n",
      "https://economictimes.indiatimes.com/markets/stocks/news/why-retail-investors-should-hold-cash-in-volatile-markets-pankaj-pandey-explains/articleshow/118773251.cms\n",
      "trading at attractive valuations compared to their historical trading range. ( disclaimer : recommendations, suggestions, views and opinions given by the experts are their own. These do not represent the views of The Eco...\n",
      "\n",
      "=== Prices (last 5 rows) ===\n",
      "HDFCBANK.NS → [{'date': Timestamp('2025-09-16 00:00:00'), 'open': 961.2000122070312, 'high': 969.3499755859375, 'low': 961.2000122070312, 'close': 966.8499755859375, 'adj_close': 966.8499755859375, 'volume': 18661794}, {'date': Timestamp('2025-09-17 00:00:00'), 'open': 965.1500244140625, 'high': 974.4000244140625, 'low': 964.3499755859375, 'close': 966.5, 'adj_close': 966.5, 'volume': 17361476}, {'date': Timestamp('2025-09-18 00:00:00'), 'open': 973.2999877929688, 'high': 979.6500244140625, 'low': 969.25, 'close': 976.9000244140625, 'adj_close': 976.9000244140625, 'volume': 17791598}, {'date': Timestamp('2025-09-19 00:00:00'), 'open': 974.9000244140625, 'high': 976.7000122070312, 'low': 962.0999755859375, 'close': 966.9000244140625, 'adj_close': 966.9000244140625, 'volume': 31243267}, {'date': Timestamp('2025-09-22 00:00:00'), 'open': 964.0, 'high': 971.2000122070312, 'low': 963.3499755859375, 'close': 965.75, 'adj_close': 965.75, 'volume': 7594885}]\n",
      "ICICIBANK.NS → [{'date': Timestamp('2025-09-16 00:00:00'), 'open': 1414.0, 'high': 1424.9000244140625, 'low': 1413.699951171875, 'close': 1421.5999755859375, 'adj_close': 1421.5999755859375, 'volume': 8213151}, {'date': Timestamp('2025-09-17 00:00:00'), 'open': 1420.0999755859375, 'high': 1426.0999755859375, 'low': 1416.4000244140625, 'close': 1419.199951171875, 'adj_close': 1419.199951171875, 'volume': 7484283}, {'date': Timestamp('2025-09-18 00:00:00'), 'open': 1431.0999755859375, 'high': 1432.800048828125, 'low': 1415.300048828125, 'close': 1421.699951171875, 'adj_close': 1421.699951171875, 'volume': 8130537}, {'date': Timestamp('2025-09-19 00:00:00'), 'open': 1416.0999755859375, 'high': 1418.0, 'low': 1400.199951171875, 'close': 1402.199951171875, 'adj_close': 1402.199951171875, 'volume': 13457344}, {'date': Timestamp('2025-09-22 00:00:00'), 'open': 1403.9000244140625, 'high': 1409.9000244140625, 'low': 1397.199951171875, 'close': 1403.300048828125, 'adj_close': 1403.300048828125, 'volume': 6294230}]\n",
      "RELIANCE.NS → [{'date': Timestamp('2025-09-16 00:00:00'), 'open': 1404.699951171875, 'high': 1408.0, 'low': 1398.199951171875, 'close': 1405.300048828125, 'adj_close': 1405.300048828125, 'volume': 8686961}, {'date': Timestamp('2025-09-17 00:00:00'), 'open': 1407.0, 'high': 1416.199951171875, 'low': 1406.9000244140625, 'close': 1413.800048828125, 'adj_close': 1413.800048828125, 'volume': 7519417}, {'date': Timestamp('2025-09-18 00:00:00'), 'open': 1420.4000244140625, 'high': 1422.0, 'low': 1410.699951171875, 'close': 1415.0, 'adj_close': 1415.0, 'volume': 9332642}, {'date': Timestamp('2025-09-19 00:00:00'), 'open': 1414.9000244140625, 'high': 1417.0, 'low': 1403.5999755859375, 'close': 1407.4000244140625, 'adj_close': 1407.4000244140625, 'volume': 13461373}, {'date': Timestamp('2025-09-22 00:00:00'), 'open': 1403.9000244140625, 'high': 1410.699951171875, 'low': 1393.300048828125, 'close': 1394.699951171875, 'adj_close': 1394.699951171875, 'volume': 5666417}]\n",
      "INFY.NS → [{'date': Timestamp('2025-09-16 00:00:00'), 'open': 1508.4000244140625, 'high': 1514.0, 'low': 1503.5, 'close': 1511.300048828125, 'adj_close': 1511.300048828125, 'volume': 8923112}, {'date': Timestamp('2025-09-17 00:00:00'), 'open': 1516.9000244140625, 'high': 1526.699951171875, 'low': 1514.0, 'close': 1522.4000244140625, 'adj_close': 1522.4000244140625, 'volume': 4795698}, {'date': Timestamp('2025-09-18 00:00:00'), 'open': 1550.0, 'high': 1555.0, 'low': 1538.5, 'close': 1540.5999755859375, 'adj_close': 1540.5999755859375, 'volume': 9331945}, {'date': Timestamp('2025-09-19 00:00:00'), 'open': 1544.5999755859375, 'high': 1551.300048828125, 'low': 1520.0999755859375, 'close': 1540.199951171875, 'adj_close': 1540.199951171875, 'volume': 12988129}, {'date': Timestamp('2025-09-22 00:00:00'), 'open': 1482.699951171875, 'high': 1514.699951171875, 'low': 1482.0, 'close': 1492.0999755859375, 'adj_close': 1492.0999755859375, 'volume': 13487684}]\n",
      "TCS.NS → [{'date': Timestamp('2025-09-16 00:00:00'), 'open': 3128.0, 'high': 3150.0, 'low': 3109.10009765625, 'close': 3145.699951171875, 'adj_close': 3145.699951171875, 'volume': 2573429}, {'date': Timestamp('2025-09-17 00:00:00'), 'open': 3152.0, 'high': 3182.39990234375, 'low': 3142.39990234375, 'close': 3172.800048828125, 'adj_close': 3172.800048828125, 'volume': 2418271}, {'date': Timestamp('2025-09-18 00:00:00'), 'open': 3185.0, 'high': 3203.0, 'low': 3161.39990234375, 'close': 3176.699951171875, 'adj_close': 3176.699951171875, 'volume': 2633138}, {'date': Timestamp('2025-09-19 00:00:00'), 'open': 3170.0, 'high': 3180.5, 'low': 3144.0, 'close': 3169.199951171875, 'adj_close': 3169.199951171875, 'volume': 4901694}, {'date': Timestamp('2025-09-22 00:00:00'), 'open': 3095.0, 'high': 3106.89990234375, 'low': 3065.0, 'close': 3067.300048828125, 'adj_close': 3067.300048828125, 'volume': 2888367}]\n",
      "\n",
      "=== RSS (sample) ===\n",
      "- SBI report backs September repo rate cut as CPI falls sharply - News Arena India -> https://news.google.com/rss/articles/CBMimwFBVV95cUxOR09CRGhtVVRKU1YzRGZpeG1sbTZmS19ETTVLeE1ZaFhPUkkxTTNQQVR1RUJ4Ry1fejhoVnpPZ09kSGFTT1BxWU8yTDBhemJMN3lfZHc4eTc3ZEhtOHlRckRicXdJWXRNTXdGZ3J6MlFyR1N5V1llQW5zeFk5eDFnbTgzdHVCOV9MZTNxckdwbndnSXU4bEdOdzV4SQ?oc=5\n",
      "- RBI may cut rates again in Dec on easing inflation: Goldman Sachs' Sengupta - Mint -> https://news.google.com/rss/articles/CBMi6wFBVV95cUxOb19RdmZoeGVNdzFOTzZRa1ZZWWRRaHdoNmdEZ0dpY0VRZlRINVNrckpTX1BOX2x6UnNhMzNTOXAybmFHU251dXFzQ1BfNm0xVjV0bUxMcEVNZ2JCV3V4NUhGVWpSUE1WUnBiZGRiOFpISWlwaFQwLXd6Uk9WYXRSM2QtemE5akRFVTVyYzRIaW5uVlhEaVB5ZG90ZXlBakVGNVNKb2N5c1NlMFg3ZEdZLThJM0pSVzRQTWlCc3N5Yk16TmpFMWxRUHVRWkZJS2F3THZWNThUSlpEZm44SzVkNFZyRHo1REpvbldJ?oc=5\n",
      "- After GST & Income Tax Relief, Will RBI MPC Offer Another Respite To Common Man Next Week? - News18 -> https://news.google.com/rss/articles/CBMi3AFBVV95cUxQR00yT3NNclhSanF6VGN2S1NZUlZyZUZCbWdMc0stQk5WMDZPSExrSHlwVF96bWpkdXcwOEtOWDZubVdEZ0lMOTlFWnNsT0JQbzFxM3ZSMC1WTWxBaGhUVGEtWkxRc19GdXdRMV85QWJnYXUzSVdJMk9kbUI1SE1TVWFSc0NTdDFRb1pSbXgxcGtCWkpJXy04dVNiYVR5aTdxdG1hV2VhbzRFaDJnRTB4OUk2bFRYOE91VHpFeG10Z2h0SllfNHJLM3oyRlBhSXJZeWE2WHY1c2VpZk9m0gHiAUFVX3lxTE5PVVlGMVU4d09WbnVnbV9DZmxyVGpUMnRkQ0JDbkM5bDNuenVxeE9pc3JoaG1nLXRLREY3YmR1Wk1qWEdlYzUtX2ZPX3hfNzFTRFl5MGJZMHF6SmpsZEJyVkRhZjFmdWNqLXRIcWVTTnJzbFNZQUlPWkJCTnQtSk9IWDNHMU5ESFc3MzhJRnA1SWhXeTAyLWloOWdkeTgwaHR0RlVXWFVOZWI4di1xdVdVZTVQbzU3UW5nUm9TS3EwS1QyUHhrdkwtblk5MDdBYklqMDVyQk1EZWt0Z1o4M2lpTUE?oc=5\n",
      "- Bankers rule out RBI rate cut in October, expect one later in FY26 - Moneycontrol -> https://news.google.com/rss/articles/CBMivwFBVV95cUxNSExUOHZNVmkweXc2bGVVMXppZHRoSUNtdkVpSHB2cnZIX29SZUpyWkRubThOd1lEemk0SVgydm91SThWTHcwd2JRLTRkMEYxSjFsQldkbmNjWk5HVkpnN2l2WHoxbGdCNkFGY1ROaDZST0NOdWdMcFhMVkVFUVBGZC1Da1R3Y3lWSE9kSjRaSUZkWXVNdUNTSkpFN0psLThOY3gyUWRNWFc4dWw5OVFiZjM0MWZGNUxoRHJtb1p5b9IBxAFBVV95cUxPRjhJYXdsdlJITnlpM3ZOeUlDaWk5X3ZCR1RwY0dHcjZBR3NnX1E4Q0NVdWE0V0hqMTVuNmt6SXNWZEcyUWh6aTdGd2kzNFJOQVdXTENwalBBaEtVdUQ5aEwzSTh4Nkp6Wkg4dTQxWmJtVU9rbEtiWndTMWpvaVFyeElNdVRVY1pwd0UtbmQwVEFLOXFrNTBQQVVaQlpPd2Myc29NVllHVndjUEtQcVNDWkRKaHZZRWVwcXotWVZfR1BuQnlX?oc=5\n",
      "- RBI keeps Repo Rate unchanged at 5.5% in August policy meet - DD News -> https://news.google.com/rss/articles/CBMijgFBVV95cUxNX3JXeHlRTVVmS1VtTDgwWC03Wkgza3JSNW8ycGNpQ1lmX1Fvcm40cnVIOUtHVFZFV1JMNmQxYmUyNWxCaVpIeUttRG9xQTFLTTNpNEVUd2w5R0c4bEZNYXMwMk5rR0h0RF93WUhMWXlxZ0NEMVZSNFFSenYwd3J5enc5V3h3QzF1Nnl6MzhR?oc=5\n",
      "\n",
      "=== Diagnostics ===\n",
      "{'index_backend': 'faiss', 'vectors': 3544, 'timing_ms': {'total': 6475, 'retrieve': 60, 'prices': 2546, 'rss': 3168}, 'errors': ['persist_fail: Object of type Timestamp is not JSON serializable']}\n"
     ]
    }
   ],
   "source": [
    "from config import NSE_TICKERS, RSS_QUERIES\n",
    "\n",
    "agent = DataAgent()\n",
    "print(agent.index.info())\n",
    "\n",
    "user_query = \"Should I buy Infosys stocks?\"\n",
    "bundle = agent.run_pipeline(user_query, tickers=NSE_TICKERS, rss_queries=RSS_QUERIES, k=10)\n",
    "\n",
    "print(\"\\n=== Retrieval (top chunks) ===\")\n",
    "evidence = bundle.get(\"evidence\", bundle.get(\"retrieval\", []))  # fallback if you still return old key\n",
    "for i, hit in enumerate(evidence[:5], 1):\n",
    "    score = hit.get(\"score\", 0.0)\n",
    "    title = hit.get(\"title\", \"\")\n",
    "    url = hit.get(\"url\", \"\")\n",
    "    text = (hit.get(\"text\", \"\") or \"\")[:220].replace(\"\\n\", \" \")\n",
    "    print(f\"[{i}] score={score:.4f} | {title}\")\n",
    "    print(url)\n",
    "    print(text + \"...\\n\")\n",
    "\n",
    "print(\"=== Prices (last 5 rows) ===\")\n",
    "market = bundle.get(\"market\", {})\n",
    "timeseries = market.get(\"timeseries\", bundle.get(\"prices\", {}))  # fallback to old key\n",
    "for sym, rows in timeseries.items():\n",
    "    print(sym, \"→\", rows)\n",
    "\n",
    "print(\"\\n=== RSS (sample) ===\")\n",
    "news = bundle.get(\"news\", {})\n",
    "rss = news.get(\"rss\", bundle.get(\"rss\", []))  # fallback to old key\n",
    "for h in rss[:5]:\n",
    "    print(\"-\", h.get(\"title\", \"\"), \"->\", h.get(\"link\", \"\"))\n",
    "\n",
    "print(\"\\n=== Diagnostics ===\")\n",
    "print(bundle.get(\"diagnostics\", {}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
